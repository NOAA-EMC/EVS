Running prologue on parent mom node: nid001014...
Job 49110209.cbqs01 nodelist: nid001014
Job 49110209.cbqs01 - Prologue complete. Execution time: 1 seconds
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=00 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110210.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=01 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110390.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=02 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110401.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=03 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110491.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=04 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110508.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=05 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110519.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=06 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110596.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=07 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110671.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=08 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110680.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=09 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110723.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=10 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110734.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=11 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110841.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=12 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49110909.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=13 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111003.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=14 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111025.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=15 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111059.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=16 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111195.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=17 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111231.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=18 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111298.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=19 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111323.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=20 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111329.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=21 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111519.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=22 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111618.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=23 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
49111626.cbqs01
++ sleep 60
++ exit
Job 49110209.cbqs01 - Epilogue complete. Execution time: 1 seconds
==================================================================
BEGIN - DEBUG INFO
==================================================================
Job 49110209.cbqs01 - Post Job Mem Usage:
nid001014: 2023-03-10.24 1 ===========================================
nid001014: 2023-03-10.24 2 #### Node Memory Usage for nid001014 ####
nid001014: 2023-03-10.24 3 ------------------
nid001014: 2023-03-10.24 4 Mem: total:527073140 used:11771572 free:516045012 shared:267656 cache:1544852 avail:515301568
nid001014: 2023-03-10.24 5 4.0K	/dev/shm
nid001014: 2023-03-10.24 6 14M	/tmp
nid001014: 2023-03-10.24 7 ===========================================
nid001014: 2023-03-10.28 1 ===========================================
nid001014: 2023-03-10.28 2 #### Node Memory Usage for nid001014 ####
nid001014: 2023-03-10.28 3 ------------------
nid001014: 2023-03-10.28 4 Mem: total:527073140 used:11337780 free:516449128 shared:267140 cache:1603900 avail:515735360
nid001014: 2023-03-10.28 5 4.0K	/dev/shm
nid001014: 2023-03-10.28 6 14M	/tmp
nid001014: 2023-03-10.28 7 ===========================================

------------------------------------------------------------------
Job 49110209.cbqs01 - /var/log/messages for job duration:
nid001014: 2023-03-10A--:--:--.------+--:-- nid001014 #### nid001014 - Job 49110209.cbqs01 Runtime Data from /var/log/messages
nid001014: 2023-03-10T16:09:23.999592+00:00 nid001014 prologue: MARK: Job 49110209.cbqs01 Start
nid001014: 2023-03-10T16:09:24.003448+00:00 nid001014 prologue: Job 49110209.cbqs01 nodelist: nid001014
nid001014: 2023-03-10T16:09:24.005428+00:00 nid001014 prologue: Job 49110209.cbqs01 - Checking existing ASLR settings...
nid001014: 2023-03-10T16:09:24.010402+00:00 nid001014 prologue: Job 49110209.cbqs01 - Checking palsd open file count...
nid001014: 2023-03-10T16:09:24.110906+00:00 nid001014 prologue: Job 49110209.cbqs01 - Checking one-shot control: False
nid001014: 2023-03-10T16:09:24.112039+00:00 nid001014 prologue: Job 49110209.cbqs01 - Recording pre-job HSN counters...
nid001014: 2023-03-10T16:09:24.228006+00:00 nid001014 prologue: Job 49110209.cbqs01 - Recording pre-job memory usage...
nid001014: 2023-03-10T16:09:24.298013+00:00 nid001014 prologue: Job 49110209.cbqs01 - Killing any stray user processes...
nid001014: 2023-03-10T16:09:24.372531+00:00 nid001014 prologue: Job 49110209.cbqs01 - Enabling turboboost...
nid001014: 2023-03-10T16:09:24.375782+00:00 nid001014 prologue: Job 49110209.cbqs01 - Verifying post-boot workarounds...
nid001014: 2023-03-10T16:09:24.670889+00:00 nid001014 prologue: Job 49110209.cbqs01 - Warchk Complete
nid001014: 2023-03-10T16:09:24.672016+00:00 nid001014 prologue: Job 49110209.cbqs01 - Recording initial NFS client statistics...
nid001014: 2023-03-10T16:09:24.679514+00:00 nid001014 prologue: Job 49110209.cbqs01 - Prologue complete. Execution time: 1 seconds
nid001014: 2023-03-10T16:09:26.026495+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=00 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:10:26.155868+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=01 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:11:26.194877+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=02 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:12:26.236128+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=03 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:13:26.299008+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=04 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:13:32.003039+00:00 nid001014 systemd[1]: etc_update.service: Succeeded.
nid001014: 2023-03-10T16:14:26.341653+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=05 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:15:26.394626+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=06 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:16:26.444996+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=07 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:17:26.486072+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=08 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:18:26.528310+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=09 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:19:26.568259+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=10 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:20:26.608435+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=11 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:21:26.649182+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=12 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:22:19.997565+00:00 nid001014 systemd[1]: etc_update.service: Succeeded.
nid001014: 2023-03-10T16:22:26.691173+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=13 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:23:26.872200+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=14 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:24:27.044212+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=15 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:25:27.084434+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=16 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:26:27.140486+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=17 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:27:27.183421+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=18 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:28:27.236941+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=19 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:29:27.275672+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=20 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:30:27.414588+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=21 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:30:59.656219+00:00 nid001014 kernel: [578470.966010] Lustre: azh2-OST001c-osc-ffff9ca255a13800: Connection to azh2-OST001c (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001014: 2023-03-10T16:30:59.680203+00:00 nid001014 kernel: [578470.986589] LNet: 229069:0:(o2iblnd_cb.c:2879:kiblnd_check_reconnect()) 10.253.190.78@o2ib: reconnect (rdma fragments), 12, 12, msg_size: 4096, queue_depth: 8/8, max_frags: 257/256
nid001014: 2023-03-10T16:30:59.680215+00:00 nid001014 kernel: [578470.986987] LNet: 27565:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Rx from 10.253.190.78@o2ib failed: 5
nid001014: 2023-03-10T16:30:59.702602+00:00 nid001014 kernel: [578471.017740] LNet: 27565:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Skipped 17 previous similar messages
nid001014: 2023-03-10T16:31:27.455360+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=22 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:31:48.808209+00:00 nid001014 kernel: [578520.115087] Lustre: azh2-OST001d-osc-ffff9ca255a13800: Connection to azh2-OST001d (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001014: 2023-03-10T16:31:50.608223+00:00 nid001014 kernel: [578521.918622] Lustre: azh2-OST001c-osc-ffff9ca255a13800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001014: 2023-03-10T16:32:01.524224+00:00 nid001014 kernel: [578532.832293] Lustre: azh2-OST001d-osc-ffff9ca255a13800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001014: 2023-03-10T16:32:27.627160+00:00 nid001014 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=23 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
nid001014: 2023-03-10T16:32:36.995294+00:00 nid001014 systemd[1]: etc_update.service: Succeeded.
nid001014: 2023-03-10T16:33:28.262936+00:00 nid001014 epilogue: Job 49110209.cbqs01 complete, running post-job actions.
nid001014: 2023-03-10T16:33:28.281703+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Recording post-job HSN counters...
nid001014: 2023-03-10T16:33:28.387405+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Recording post-job memory usage...
nid001014: 2023-03-10T16:33:28.406234+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Recording job NFS statistics to /tmp/nfsstats.49110209.cbqs01
nid001014: 2023-03-10T16:33:28.892358+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Clearing /tmp...
nid001014: 2023-03-10T16:33:28.913965+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Clearing shared memory...
nid001014: 2023-03-10T16:33:28.926332+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Clearing memory cache...
nid001014: 2023-03-10T16:33:29.060557+00:00 nid001014 kernel: [578620.369515] drop_caches (25939): drop_caches: 3
nid001014: 2023-03-10T16:33:29.166325+00:00 nid001014 epilogue: Job 49110209.cbqs01 - Releasing Lustre Locks...
nid001014: 2023-03-10T16:33:29.182368+00:00 nid001014 epilogue: MARK: Job 49110209.cbqs01 Complete.

------------------------------------------------------------------
Job 49110209.cbqs01 - dmesg output for job duration:
nid001014: 2023-03-10A--:--:--.------+--:-- nid001014 #### nid001014 - Job 49110209.cbqs01 Runtime Data from dmesg
nid001014: [Fri Mar 10 16:08:52 2023] MARK: Job 49110209.cbqs01 Start
nid001014: [Fri Mar 10 16:30:27 2023] LNet: 27565:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Rx from 10.253.190.78@o2ib failed: 5
nid001014: [Fri Mar 10 16:30:27 2023] LNet: 229069:0:(o2iblnd_cb.c:2879:kiblnd_check_reconnect()) 10.253.190.78@o2ib: reconnect (rdma fragments), 12, 12, msg_size: 4096, queue_depth: 8/8, max_frags: 257/256
nid001014: [Fri Mar 10 16:30:27 2023] Lustre: azh2-OST001c-osc-ffff9ca255a13800: Connection to azh2-OST001c (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001014: [Fri Mar 10 16:30:28 2023] LNet: 27565:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Skipped 17 previous similar messages
nid001014: [Fri Mar 10 16:31:17 2023] Lustre: azh2-OST001d-osc-ffff9ca255a13800: Connection to azh2-OST001d (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001014: [Fri Mar 10 16:31:18 2023] Lustre: azh2-OST001c-osc-ffff9ca255a13800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001014: [Fri Mar 10 16:31:29 2023] Lustre: azh2-OST001d-osc-ffff9ca255a13800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001014: [Fri Mar 10 16:32:57 2023] MARK: Job 49110209.cbqs01 Complete.
nid001014: [Fri Mar 10 16:32:57 2023] drop_caches (25939): drop_caches: 3

------------------------------------------------------------------
Job 49110209.cbqs01 - Pre/Post job diff on HSN (MLX) Counters:
nid001014: 2023-03-10A--:--:--.------+--:-- nid001014 #### nid001014 - Job 49110209.cbqs01 HSN0 MLX Counter Post-Job Difference
nid001014: multicast: 256578348					      |	multicast: 256582189
nid001014: port_rcv_data: 14496898854228				      |	port_rcv_data: 14496899683328
nid001014: port_rcv_packets: 21820691765				      |	port_rcv_packets: 21820705951
nid001014: port_xmit_data: 15630828788700				      |	port_xmit_data: 15630829636200
nid001014: port_xmit_packets: 23023443579				      |	port_xmit_packets: 23023457813
nid001014: resp_cqe_error: 9802					      |	resp_cqe_error: 9820
nid001014: resp_cqe_flush_error: 9490				      |	resp_cqe_flush_error: 9507
nid001014: rx_bytes: 45258193271					      |	rx_bytes: 45259239833
nid001014: rx_packets: 291803414					      |	rx_packets: 291809875
nid001014: tx_bytes: 208866764980					      |	tx_bytes: 208867434829
nid001014: tx_packets: 47941453					      |	tx_packets: 47945106
nid001014: unicast_rcv_packets: 21820691765			      |	unicast_rcv_packets: 21820705951
nid001014: unicast_xmit_packets: 23023443579			      |	unicast_xmit_packets: 23023457813

------------------------------------------------------------------
### Job 49110209.cbqs01 - NFS Statistics for job duration:
nid001014: #### 2023-03-10.00 #### nid001014 - Job 49110209.cbqs01 NFS Statistics for job duration (nfsstat)
nid001014: ## /usr/sbin/nfsstat -v -S /tmp/nfsstats.begin.49110209.cbqs01 :
nid001014: Client packet stats:
nid001014: packets    udp        tcp        tcpconn
nid001014: 0          0          0          0       
nid001014: 
nid001014: Client rpc stats:
nid001014: calls      retrans    authrefrsh
nid001014: 2774       0          2774    
nid001014: 
nid001014: Client nfs v3:
nid001014: null             getattr          setattr          lookup           access           
nid001014: 0         0%     1681     60%     3         0%     359      12%     297      10%     
nid001014: readlink         read             write            create           mkdir            
nid001014: 30        1%     266       9%     36        1%     0         0%     0         0%     
nid001014: symlink          mknod            remove           rmdir            rename           
nid001014: 0         0%     0         0%     0         0%     0         0%     0         0%     
nid001014: link             readdir          readdirplus      fsstat           fsinfo           
nid001014: 0         0%     0         0%     102       3%     0         0%     0         0%     
nid001014: pathconf         commit           
nid001014: 0         0%     0         0%     
nid001014: 
nid001014: 
nid001014: -----------------------------------------------------------------------------------
nid001014: 
nid001014: #### 2023-03-10.00 #### nid001014 - Job 49110209.cbqs01 NFS Mount Statistics for job duration (mountstats)
nid001014: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110209.cbqs01 /apps :
nid001014: 
nid001014: 
nid001014: 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001014: 
nid001014:            ops/s       rpc bklog
nid001014:            0.284           0.000
nid001014: 
nid001014: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.059           0.149           2.526        0 (0.0%)           0.212           0.247
nid001014: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001014: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110209.cbqs01 /sfs :
nid001014: 
nid001014: 
nid001014: 172.20.250.17:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001014: 
nid001014:            ops/s       rpc bklog
nid001014:            0.055           0.000
nid001014: 
nid001014: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.001           0.000           0.391        0 (0.0%)           1.000           1.000
nid001014: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.025           0.057           2.284        0 (0.0%)           0.361           0.444
nid001014: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110209.cbqs01 /u :
nid001014: 
nid001014: 
nid001014: 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001014: 
nid001014:            ops/s       rpc bklog
nid001014:            0.055           0.000
nid001014: 
nid001014: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.001           0.001           1.094        0 (0.0%)           0.000           0.000
nid001014: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001014: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110209.cbqs01 /pe :
nid001014: 
nid001014: 
nid001014: 10.31.62.243:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001014: 
nid001014:            ops/s       rpc bklog
nid001014:            0.982           0.000
nid001014: 
nid001014: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.006           0.031           5.680        0 (0.0%)           0.375           0.375
nid001014: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001014:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001014: ----------------
nid001014: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110209.cbqs01 /apps :
nid001014: Stats for 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001014:   NFS mount options: ro,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.16,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001014:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001014:   NFS security flavor: 1  pseudoflavor: 0
nid001014: 
nid001014: NFS byte counts:
nid001014:   applications read 497530 bytes via read(2)
nid001014:   applications wrote 0 bytes via write(2)
nid001014:   applications read 0 bytes via O_DIRECT read(2)
nid001014:   applications wrote 0 bytes via O_DIRECT write(2)
nid001014:   client read 195898 bytes via NFS READ
nid001014:   client wrote 0 bytes via NFS WRITE
nid001014: 
nid001014: RPC statistics:
nid001014:   410 RPC requests sent, 410 RPC replies received (0 XIDs not found)
nid001014:   average backlog queue length: 0
nid001014: 
nid001014: GETATTR:
nid001014: 	718 ops (175%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 112
nid001014: 	backlog wait: 0.001393 	RTT: 0.097493 	total execute time: 0.110028 (milliseconds)
nid001014: ACCESS:
nid001014: 	214 ops (52%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 120
nid001014: 	backlog wait: 0.000000 	RTT: 0.294393 	total execute time: 0.303738 (milliseconds)
nid001014: READDIRPLUS:
nid001014: 	102 ops (24%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 876
nid001014: 	backlog wait: 0.000000 	RTT: 0.098039 	total execute time: 0.107843 (milliseconds)
nid001014: READ:
nid001014: 	85 ops (20%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 2434
nid001014: 	backlog wait: 0.023529 	RTT: 0.211765 	total execute time: 0.247059 (milliseconds)
nid001014: LOOKUP:
nid001014: 	83 ops (20%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 169
nid001014: 	backlog wait: 0.000000 	RTT: 0.120482 	total execute time: 0.120482 (milliseconds)
nid001014: READLINK:
nid001014: 	28 ops (6%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 137
nid001014: 	backlog wait: 0.000000 	RTT: 0.071429 	total execute time: 0.107143 (milliseconds)
nid001014: 
nid001014: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110209.cbqs01 /sfs :
nid001014: 
nid001014: Stats for 172.20.250.17:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001014:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001014:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001014:   NFS security flavor: 1  pseudoflavor: 0
nid001014: 
nid001014: NFS byte counts:
nid001014:   applications read 13993 bytes via read(2)
nid001014:   applications wrote 9227 bytes via write(2)
nid001014:   applications read 0 bytes via O_DIRECT read(2)
nid001014:   applications wrote 0 bytes via O_DIRECT write(2)
nid001014:   client read 146 bytes via NFS READ
nid001014:   client wrote 73482 bytes via NFS WRITE
nid001014: 
nid001014: RPC statistics:
nid001014:   80 RPC requests sent, 80 RPC replies received (0 XIDs not found)
nid001014:   average backlog queue length: 0
nid001014: 
nid001014: GETATTR:
nid001014: 	86 ops (107%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 112
nid001014: 	backlog wait: 0.000000 	RTT: 0.127907 	total execute time: 0.162791 (milliseconds)
nid001014: WRITE:
nid001014: 	36 ops (45%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 160
nid001014: 	backlog wait: 0.055556 	RTT: 0.361111 	total execute time: 0.444444 (milliseconds)
nid001014: ACCESS:
nid001014: 	21 ops (26%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 120
nid001014: 	backlog wait: 0.047619 	RTT: 0.285714 	total execute time: 0.285714 (milliseconds)
nid001014: LOOKUP:
nid001014: 	5 ops (6%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 240
nid001014: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.200000 (milliseconds)
nid001014: SETATTR:
nid001014: 	3 ops (3%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 144
nid001014: 	backlog wait: 0.000000 	RTT: 0.333333 	total execute time: 0.333333 (milliseconds)
nid001014: READ:
nid001014: 	1 ops (1%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 276
nid001014: 	backlog wait: 0.000000 	RTT: 1.000000 	total execute time: 1.000000 (milliseconds)
nid001014: 
nid001014: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110209.cbqs01 /u :
nid001014: 
nid001014: Stats for 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001014:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001014:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001014:   NFS security flavor: 1  pseudoflavor: 0
nid001014: 
nid001014: NFS byte counts:
nid001014:   applications read 837 bytes via read(2)
nid001014:   applications wrote 0 bytes via write(2)
nid001014:   applications read 0 bytes via O_DIRECT read(2)
nid001014:   applications wrote 0 bytes via O_DIRECT write(2)
nid001014:   client read 837 bytes via NFS READ
nid001014:   client wrote 0 bytes via NFS WRITE
nid001014: 
nid001014: RPC statistics:
nid001014:   80 RPC requests sent, 80 RPC replies received (0 XIDs not found)
nid001014:   average backlog queue length: 0
nid001014: 
nid001014: GETATTR:
nid001014: 	48 ops (60%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 112
nid001014: 	backlog wait: 0.000000 	RTT: 0.229167 	total execute time: 0.250000 (milliseconds)
nid001014: ACCESS:
nid001014: 	28 ops (35%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 120
nid001014: 	backlog wait: 0.000000 	RTT: 0.107143 	total execute time: 0.178571 (milliseconds)
nid001014: LOOKUP:
nid001014: 	12 ops (15%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 136
nid001014: 	backlog wait: 0.000000 	RTT: 0.166667 	total execute time: 0.166667 (milliseconds)
nid001014: READ:
nid001014: 	1 ops (1%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 968
nid001014: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.000000 (milliseconds)
nid001014: 
nid001014: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110209.cbqs01 /pe :
nid001014: 
nid001014: Stats for 10.31.62.243:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001014:   NFS mount options: ro,vers=3,rsize=1048576,wsize=1048576,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,nolock,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.31.62.243,mountvers=3,mountport=38465,mountproto=tcp,local_lock=all
nid001014:   NFS server capabilities: caps=0x3fcf,wtmult=4096,dtsize=32768,bsize=0,namlen=255
nid001014:   NFS security flavor: 1  pseudoflavor: 0
nid001014: 
nid001014: NFS byte counts:
nid001014:   applications read 20480 bytes via read(2)
nid001014:   applications wrote 0 bytes via write(2)
nid001014:   applications read 0 bytes via O_DIRECT read(2)
nid001014:   applications wrote 0 bytes via O_DIRECT write(2)
nid001014:   client read 45056 bytes via NFS READ
nid001014:   client wrote 0 bytes via NFS WRITE
nid001014: 
nid001014: RPC statistics:
nid001014:   1418 RPC requests sent, 1418 RPC replies received (0 XIDs not found)
nid001014:   average backlog queue length: 0
nid001014: 
nid001014: GETATTR:
nid001014: 	828 ops (58%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 112
nid001014: 	backlog wait: 0.002415 	RTT: 0.481884 	total execute time: 0.495169 (milliseconds)
nid001014: LOOKUP:
nid001014: 	259 ops (18%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 62
nid001014: 	backlog wait: 0.000000 	RTT: 0.594595 	total execute time: 0.606178 (milliseconds)
nid001014: ACCESS:
nid001014: 	34 ops (2%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 36
nid001014: 	backlog wait: 0.000000 	RTT: 0.323529 	total execute time: 0.352941 (milliseconds)
nid001014: READ:
nid001014: 	8 ops (0%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 5676
nid001014: 	backlog wait: 0.000000 	RTT: 0.375000 	total execute time: 0.375000 (milliseconds)
nid001014: READLINK:
nid001014: 	2 ops (0%) 
nid001014: 	avg bytes sent per op: avg bytes received per op: 136
nid001014: 	backlog wait: 0.000000 	RTT: 0.500000 	total execute time: 0.500000 (milliseconds)
nid001014: 
nid001014: 

------------------------------------------------------------------
### Job 49110209.cbqs01 - Exit status is 0

------------------------------------------------------------------
Job 49110209.cbqs01 - Job summary:
Job Id: 49110209.cbqs01
    Job_Name = run_rtma_ru_stats
    Job_Owner = emc.vpppg@clogin03.cactus.wcoss2.ncep.noaa.gov
    resources_used.cpupercent = 0
    resources_used.cput = 00:00:01
    resources_used.mem = 22896kb
    resources_used.ncpus = 1
    resources_used.vmem = 39096kb
    resources_used.walltime = 00:23:08
    job_state = R
    queue = dev
    server = cbqs01
    Account_Name = EVS-DEV
    Checkpoint = u
    ctime = Fri Mar 10 16:09:22 2023
    Error_Path = clogin03.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/noscrub
	/emc.vpppg/EVS/ecf/realtime_analyses/stats/run_rtma_ru_stats.e49110209
    exec_host = nid001014/0
    exec_vnode = (nid001014:ncpus=1:mem=2097152kb)
    Hold_Types = n
    Join_Path = oe
    Keep_Files = oed
    Mail_Points = a
    mtime = Fri Mar 10 16:32:34 2023
    Output_Path = clogin03.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/noscru
	b/emc.vpppg/EVS/ecf/realtime_analyses/stats/run_rtma_ru_stats.o49110209
	
    Priority = 0
    qtime = Fri Mar 10 16:09:22 2023
    Rerunable = False
    Resource_List.alvl = 2
    Resource_List.aslr = True
    Resource_List.debug = True
    Resource_List.dfs = False
    Resource_List.hyper = True
    Resource_List.mem = 2gb
    Resource_List.ncpus = 1
    Resource_List.nodect = 1
    Resource_List.one-shot = False
    Resource_List.place = shared
    Resource_List.select = 1:ncpus=1:mem=2GB
    Resource_List.thp = True
    Resource_List.turbo = True
    Resource_List.walltime = 02:00:00
    schedselect = 1:ncpus=1:mem=2GB:prepost=False
    stime = Fri Mar 10 16:09:23 2023
    session_id = 24980
    Shell_Path_List = /bin/bash
    jobdir = /u/emc.vpppg
    substate = 42
    Variable_List = PBS_O_HOME=/u/emc.vpppg,PBS_O_LANG=en_US.UTF-8,
	PBS_O_LOGNAME=emc.vpppg,
	PBS_O_PATH=/apps/spack/python/3.8.6/intel/19.1.3.304/pjn2nzkjvqgmjw4hm
	yz43v5x4jbxjzpk/bin:/pe/intel/compilers_and_libraries_2020.4.304/linux/
	bin/intel64:/pe/intel/compilers_and_libraries_2020.4.304/linux/bin:/pe/
	intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/pe/inte
	l/debugger_2020/gdb/intel64/bin:/opt/cray/libfabric/1.11.0.0./bin:/opt/
	clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/us
	r/bin:/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/apps/
	prod/python-modules/3.8.6/intel/19.1.3.304/bin:/apps/prod/python-module
	s/3.8.6/intel/19.1.3.304/lib/python3.8/site-packages/bin,
	PBS_O_MAIL=/var/mail/emc.vpppg,PBS_O_SHELL=/bin/bash,
	PBS_O_WORKDIR=/lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_ana
	lyses/stats,PBS_O_SYSTEM=Linux,PBS_O_QUEUE=dev,
	PBS_O_HOST=clogin03.cactus.wcoss2.ncep.noaa.gov
    euser = emc.vpppg
    egroup = emc
    hashname = 49110209.cbqs01
    queue_rank = 1678464562694510
    queue_type = E
    comment = Job run at Fri Mar 10 at 16:09 on (nid001014:ncpus=1:mem=2097152k
	b)
    etime = Fri Mar 10 16:09:22 2023
    umask = 22
    run_count = 6
    eligible_time = 00:00:04
    accrue_type = 3
    Submit_arguments = run_rtma_ru_stats.sh
    project = EVS-DEV
    run_version = 1
    Submit_Host = clogin03.cactus.wcoss2.ncep.noaa.gov
    server_instance_id = cbqs01.cactus.wcoss2.ncep.noaa.gov:15001


------------------------------------------------------------------
Job 49110209.cbqs01 - PBS tracejob output (for parent mom node only):

Job: 49110209.cbqs01

03/10/2023 16:09:23  M    update_job_usage: CPU usage: 0.000 secs
03/10/2023 16:09:23  M    update_job_usage: cpupercent initialized to zero
03/10/2023 16:09:23  M    update_job_usage: Memory usage: mem=0b
03/10/2023 16:09:24  M    Started, pid = 24980
03/10/2023 16:10:58  M    update_job_usage: CPU usage: 0.743 secs
03/10/2023 16:10:58  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:13:00  M    update_job_usage: CPU usage: 0.780 secs
03/10/2023 16:13:00  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:15:01  M    update_job_usage: CPU usage: 0.817 secs
03/10/2023 16:15:01  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:17:03  M    update_job_usage: CPU usage: 0.855 secs
03/10/2023 16:17:03  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:19:04  M    update_job_usage: CPU usage: 0.892 secs
03/10/2023 16:19:04  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:21:05  M    update_job_usage: CPU usage: 0.928 secs
03/10/2023 16:21:05  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:23:06  M    update_job_usage: CPU usage: 0.967 secs
03/10/2023 16:23:06  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:25:07  M    update_job_usage: CPU usage: 1.005 secs
03/10/2023 16:25:07  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:27:08  M    update_job_usage: CPU usage: 1.041 secs
03/10/2023 16:27:08  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:29:09  M    update_job_usage: CPU usage: 1.077 secs
03/10/2023 16:29:09  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:31:10  M    update_job_usage: CPU usage: 1.113 secs
03/10/2023 16:31:10  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:33:12  M    update_job_usage: CPU usage: 1.150 secs
03/10/2023 16:33:12  M    update_job_usage: Memory usage: mem=22896kb
03/10/2023 16:33:27  M    task 00000001 terminated
03/10/2023 16:33:27  M    Terminated
03/10/2023 16:33:27  M    task 00000001 cput=00:00:02
03/10/2023 16:33:27  M    kill_job
03/10/2023 16:33:27  M    nid001014 cput=00:00:01 mem=22896kb
03/10/2023 16:33:27  M    update_job_usage: CPU usage: 1.151 secs
03/10/2023 16:33:27  M    update_job_usage: Memory usage: mem=22896kb

------------------------------------------------------------------
To see full PBS log data, run: /sfs/admin/scripts/tracejob.sh 49110209.cbqs01

==================================================================
END - DEBUG INFO
==================================================================

##### Job 49110209.cbqs01 - PBS Job Script:

#PBS -N run_rtma_ru_stats
#PBS -j oe
#PBS -S /bin/bash
#PBS -q "dev"
#PBS -A EVS-DEV
#PBS -l walltime=02:00:00
#PBS -l select=1:ncpus=1:mem=2GB
#PBS -l debug=true

set -x

for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
do
   export fhr
   qsub -v cyc=$fhr /lfs/h2/emc/vpppg/noscrub/$USER/EVS/ecf/realtime_analyses/stats/jevs_rtma_ru_stats.ecf
   sleep 60
done

exit

##### End of job script
------------------------------------------------------------------
