Running prologue on parent mom node: nid001070...
Job 49110212.cbqs01 nodelist: nid001070
Job 49110212.cbqs01 - Prologue complete. Execution time: 1 seconds
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=00 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110219.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=01 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110393.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=02 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110402.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=03 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110493.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=04 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110510.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=05 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110521.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=06 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110615.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=07 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110673.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=08 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110682.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=09 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110726.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=10 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110735.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=11 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110889.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=12 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49110910.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=13 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111005.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=14 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111026.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=15 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111065.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=16 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111202.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=17 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111232.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=18 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111302.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=19 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111324.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=20 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111330.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=21 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111539.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=22 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111619.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=23 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
49111628.cbqs01
++ sleep 60
++ exit
Job 49110212.cbqs01 - Epilogue complete. Execution time: 2 seconds
==================================================================
BEGIN - DEBUG INFO
==================================================================
Job 49110212.cbqs01 - Post Job Mem Usage:
nid001070: 2023-03-10.29 1 ===========================================
nid001070: 2023-03-10.29 2 #### Node Memory Usage for nid001070 ####
nid001070: 2023-03-10.29 3 ------------------
nid001070: 2023-03-10.29 4 Mem: total:527073140 used:11627252 free:516119256 shared:321636 cache:1739712 avail:515445888
nid001070: 2023-03-10.29 5 4.0K	/dev/shm
nid001070: 2023-03-10.29 6 14M	/tmp
nid001070: 2023-03-10.29 7 ===========================================
nid001070: 2023-03-10.34 1 ===========================================
nid001070: 2023-03-10.34 2 #### Node Memory Usage for nid001070 ####
nid001070: 2023-03-10.34 3 ------------------
nid001070: 2023-03-10.34 4 Mem: total:527073140 used:11656720 free:516073948 shared:320508 cache:1770144 avail:515416420
nid001070: 2023-03-10.34 5 4.0K	/dev/shm
nid001070: 2023-03-10.34 6 14M	/tmp
nid001070: 2023-03-10.34 7 ===========================================

------------------------------------------------------------------
Job 49110212.cbqs01 - /var/log/messages for job duration:
nid001070: 2023-03-10A--:--:--.------+--:-- nid001070 #### nid001070 - Job 49110212.cbqs01 Runtime Data from /var/log/messages
nid001070: 2023-03-10T16:09:28.520569+00:00 nid001070 prologue: MARK: Job 49110212.cbqs01 Start
nid001070: 2023-03-10T16:09:28.525372+00:00 nid001070 prologue: Job 49110212.cbqs01 nodelist: nid001070
nid001070: 2023-03-10T16:09:28.526482+00:00 nid001070 prologue: Job 49110212.cbqs01 - Checking existing ASLR settings...
nid001070: 2023-03-10T16:09:28.533671+00:00 nid001070 prologue: Job 49110212.cbqs01 - Checking palsd open file count...
nid001070: 2023-03-10T16:09:28.969471+00:00 nid001070 prologue: Job 49110212.cbqs01 - Checking one-shot control: False
nid001070: 2023-03-10T16:09:28.970548+00:00 nid001070 prologue: Job 49110212.cbqs01 - Recording pre-job HSN counters...
nid001070: 2023-03-10T16:09:29.074928+00:00 nid001070 prologue: Job 49110212.cbqs01 - Recording pre-job memory usage...
nid001070: 2023-03-10T16:09:29.107372+00:00 nid001070 prologue: Job 49110212.cbqs01 - Killing any stray user processes...
nid001070: 2023-03-10T16:09:29.179446+00:00 nid001070 prologue: Job 49110212.cbqs01 - Enabling turboboost...
nid001070: 2023-03-10T16:09:29.182065+00:00 nid001070 prologue: Job 49110212.cbqs01 - Verifying post-boot workarounds...
nid001070: 2023-03-10T16:09:29.476404+00:00 nid001070 prologue: Job 49110212.cbqs01 - Warchk Complete
nid001070: 2023-03-10T16:09:29.477495+00:00 nid001070 prologue: Job 49110212.cbqs01 - Recording initial NFS client statistics...
nid001070: 2023-03-10T16:09:29.484248+00:00 nid001070 prologue: Job 49110212.cbqs01 - Prologue complete. Execution time: 1 seconds
nid001070: 2023-03-10T16:09:30.696915+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=00 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:10:30.792881+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=01 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:10:45.678941+00:00 nid001070 systemd[1]: etc_update.service: Succeeded.
nid001070: 2023-03-10T16:11:30.834132+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=02 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:12:30.875447+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=03 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:13:30.950971+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=04 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:14:30.989930+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=05 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:15:31.030303+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=06 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:16:31.079031+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=07 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:17:31.134497+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=08 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:18:31.176394+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=09 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:19:31.216898+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=10 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:20:19.674730+00:00 nid001070 systemd[1]: etc_update.service: Succeeded.
nid001070: 2023-03-10T16:20:31.327502+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=11 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:21:33.414208+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=12 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:22:33.640922+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=13 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:23:33.682248+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=14 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:24:33.721611+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=15 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:25:33.761952+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=16 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:26:33.877875+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=17 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:27:33.946602+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=18 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:28:33.994451+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=19 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:29:34.034689+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=20 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:30:34.077854+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=21 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:30:56.971404+00:00 nid001070 kernel: [431412.799570] Lustre: azh2-OST001c-osc-ffff9faeb6463800: Connection to azh2-OST001c (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001070: 2023-03-10T16:30:56.987421+00:00 nid001070 kernel: [431412.816391] LNet: 177486:0:(o2iblnd_cb.c:2879:kiblnd_check_reconnect()) 10.253.190.78@o2ib: reconnect (rdma fragments), 12, 12, msg_size: 4096, queue_depth: 8/8, max_frags: 257/256
nid001070: 2023-03-10T16:30:56.987433+00:00 nid001070 kernel: [431412.816722] LNet: 12461:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Rx from 10.253.190.78@o2ib failed: 5
nid001070: 2023-03-10T16:30:56.999555+00:00 nid001070 kernel: [431412.835112] LNet: 12461:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Skipped 17 previous similar messages
nid001070: 2023-03-10T16:31:15.674801+00:00 nid001070 systemd[1]: etc_update.service: Succeeded.
nid001070: 2023-03-10T16:31:34.139845+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=22 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:31:46.123403+00:00 nid001070 kernel: [431461.950302] Lustre: azh2-OST001d-osc-ffff9faeb6463800: Connection to azh2-OST001d (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001070: 2023-03-10T16:31:50.631399+00:00 nid001070 kernel: [431466.464731] Lustre: azh2-OST001c-osc-ffff9faeb6463800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001070: 2023-03-10T16:32:01.511400+00:00 nid001070 kernel: [431477.344377] Lustre: azh2-OST001d-osc-ffff9faeb6463800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001070: 2023-03-10T16:32:34.235925+00:00 nid001070 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=23 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
nid001070: 2023-03-10T16:33:34.862914+00:00 nid001070 epilogue: Job 49110212.cbqs01 complete, running post-job actions.
nid001070: 2023-03-10T16:33:34.879101+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Recording post-job HSN counters...
nid001070: 2023-03-10T16:33:34.994151+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Recording post-job memory usage...
nid001070: 2023-03-10T16:33:35.011240+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Recording job NFS statistics to /tmp/nfsstats.49110212.cbqs01
nid001070: 2023-03-10T16:33:35.552741+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Clearing /tmp...
nid001070: 2023-03-10T16:33:35.572636+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Clearing shared memory...
nid001070: 2023-03-10T16:33:35.584998+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Clearing memory cache...
nid001070: 2023-03-10T16:33:35.731998+00:00 nid001070 kernel: [431571.563503] drop_caches (179327): drop_caches: 3
nid001070: 2023-03-10T16:33:35.943694+00:00 nid001070 epilogue: Job 49110212.cbqs01 - Releasing Lustre Locks...
nid001070: 2023-03-10T16:33:35.958192+00:00 nid001070 epilogue: MARK: Job 49110212.cbqs01 Complete.

------------------------------------------------------------------
Job 49110212.cbqs01 - dmesg output for job duration:
nid001070: 2023-03-10A--:--:--.------+--:-- nid001070 #### nid001070 - Job 49110212.cbqs01 Runtime Data from dmesg
nid001070: [Fri Mar 10 16:09:24 2023] MARK: Job 49110212.cbqs01 Start
nid001070: [Fri Mar 10 16:30:52 2023] LNet: 12461:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Rx from 10.253.190.78@o2ib failed: 5
nid001070: [Fri Mar 10 16:30:52 2023] LNet: 12461:0:(o2iblnd_cb.c:507:kiblnd_rx_complete()) Skipped 17 previous similar messages
nid001070: [Fri Mar 10 16:30:52 2023] LNet: 177486:0:(o2iblnd_cb.c:2879:kiblnd_check_reconnect()) 10.253.190.78@o2ib: reconnect (rdma fragments), 12, 12, msg_size: 4096, queue_depth: 8/8, max_frags: 257/256
nid001070: [Fri Mar 10 16:30:52 2023] Lustre: azh2-OST001c-osc-ffff9faeb6463800: Connection to azh2-OST001c (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001070: [Fri Mar 10 16:31:41 2023] Lustre: azh2-OST001d-osc-ffff9faeb6463800: Connection to azh2-OST001d (at 10.253.190.77@o2ib) was lost; in progress operations using this service will wait for recovery to complete
nid001070: [Fri Mar 10 16:31:46 2023] Lustre: azh2-OST001c-osc-ffff9faeb6463800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001070: [Fri Mar 10 16:31:57 2023] Lustre: azh2-OST001d-osc-ffff9faeb6463800: Connection restored to 10.253.190.78@o2ib (at 10.253.190.78@o2ib)
nid001070: [Fri Mar 10 16:33:31 2023] MARK: Job 49110212.cbqs01 Complete.
nid001070: [Fri Mar 10 16:33:31 2023] drop_caches (179327): drop_caches: 3

------------------------------------------------------------------
Job 49110212.cbqs01 - Pre/Post job diff on HSN (MLX) Counters:
nid001070: 2023-03-10A--:--:--.------+--:-- nid001070 #### nid001070 - Job 49110212.cbqs01 HSN0 MLX Counter Post-Job Difference
nid001070: multicast: 256087449					      |	multicast: 256091293
nid001070: port_rcv_data: 23330028273002				      |	port_rcv_data: 23330029076401
nid001070: port_rcv_packets: 31749630736				      |	port_rcv_packets: 31749644406
nid001070: port_xmit_data: 23035554438163				      |	port_xmit_data: 23035555259872
nid001070: port_xmit_packets: 31433129290				      |	port_xmit_packets: 31433143013
nid001070: resp_cqe_error: 53057					      |	resp_cqe_error: 53075
nid001070: resp_cqe_flush_error: 50317				      |	resp_cqe_flush_error: 50334
nid001070: rx_bytes: 59700630313					      |	rx_bytes: 59701553993
nid001070: rx_packets: 289566329					      |	rx_packets: 289572662
nid001070: tx_bytes: 112468726358					      |	tx_bytes: 112469391917
nid001070: tx_packets: 33618679					      |	tx_packets: 33622277
nid001070: unicast_rcv_packets: 31749630736			      |	unicast_rcv_packets: 31749644406
nid001070: unicast_xmit_packets: 31433129290			      |	unicast_xmit_packets: 31433143013

------------------------------------------------------------------
### Job 49110212.cbqs01 - NFS Statistics for job duration:
nid001070: #### 2023-03-10.00 #### nid001070 - Job 49110212.cbqs01 NFS Statistics for job duration (nfsstat)
nid001070: ## /usr/sbin/nfsstat -v -S /tmp/nfsstats.begin.49110212.cbqs01 :
nid001070: Client packet stats:
nid001070: packets    udp        tcp        tcpconn
nid001070: 0          0          0          0       
nid001070: 
nid001070: Client rpc stats:
nid001070: calls      retrans    authrefrsh
nid001070: 2603       0          2603    
nid001070: 
nid001070: Client nfs v3:
nid001070: null             getattr          setattr          lookup           access           
nid001070: 0         0%     1731     66%     3         0%     323      12%     297      11%     
nid001070: readlink         read             write            create           mkdir            
nid001070: 23        0%     94        3%     36        1%     0         0%     0         0%     
nid001070: symlink          mknod            remove           rmdir            rename           
nid001070: 0         0%     0         0%     0         0%     0         0%     0         0%     
nid001070: link             readdir          readdirplus      fsstat           fsinfo           
nid001070: 0         0%     0         0%     96        3%     0         0%     0         0%     
nid001070: pathconf         commit           
nid001070: 0         0%     0         0%     
nid001070: 
nid001070: 
nid001070: -----------------------------------------------------------------------------------
nid001070: 
nid001070: #### 2023-03-10.00 #### nid001070 - Job 49110212.cbqs01 NFS Mount Statistics for job duration (mountstats)
nid001070: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110212.cbqs01 /apps :
nid001070: 
nid001070: 
nid001070: 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001070: 
nid001070:            ops/s       rpc bklog
nid001070:            0.275           0.000
nid001070: 
nid001070: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.032           0.071           2.241        0 (0.0%)           0.196           0.239
nid001070: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001070: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110212.cbqs01 /sfs :
nid001070: 
nid001070: 
nid001070: 172.20.250.17:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001070: 
nid001070:            ops/s       rpc bklog
nid001070:            0.056           0.000
nid001070: 
nid001070: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.001           0.003           2.223        0 (0.0%)           0.500           0.500
nid001070: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.025           0.060           2.428        0 (0.0%)           3.222           3.278
nid001070: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110212.cbqs01 /u :
nid001070: 
nid001070: 
nid001070: 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001070: 
nid001070:            ops/s       rpc bklog
nid001070:            0.056           0.000
nid001070: 
nid001070: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.001           0.001           1.094        0 (0.0%)           0.000           0.000
nid001070: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001070: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49110212.cbqs01 /pe :
nid001070: 
nid001070: 
nid001070: 10.31.62.243:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001070: 
nid001070:            ops/s       rpc bklog
nid001070:            0.886           0.000
nid001070: 
nid001070: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.001           0.014          10.180        0 (0.0%)           0.000           0.500
nid001070: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001070:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001070: ----------------
nid001070: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110212.cbqs01 /apps :
nid001070: Stats for 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001070:   NFS mount options: ro,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.16,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001070:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001070:   NFS security flavor: 1  pseudoflavor: 0
nid001070: 
nid001070: NFS byte counts:
nid001070:   applications read 497530 bytes via read(2)
nid001070:   applications wrote 0 bytes via write(2)
nid001070:   applications read 0 bytes via O_DIRECT read(2)
nid001070:   applications wrote 0 bytes via O_DIRECT write(2)
nid001070:   client read 92619 bytes via NFS READ
nid001070:   client wrote 0 bytes via NFS WRITE
nid001070: 
nid001070: RPC statistics:
nid001070:   397 RPC requests sent, 397 RPC replies received (0 XIDs not found)
nid001070:   average backlog queue length: 0
nid001070: 
nid001070: GETATTR:
nid001070: 	768 ops (193%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 112
nid001070: 	backlog wait: 0.001302 	RTT: 0.089844 	total execute time: 0.102865 (milliseconds)
nid001070: ACCESS:
nid001070: 	214 ops (53%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 120
nid001070: 	backlog wait: 0.000000 	RTT: 0.336449 	total execute time: 0.350467 (milliseconds)
nid001070: READDIRPLUS:
nid001070: 	96 ops (24%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 884
nid001070: 	backlog wait: 0.000000 	RTT: 0.104167 	total execute time: 0.114583 (milliseconds)
nid001070: LOOKUP:
nid001070: 	47 ops (11%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 163
nid001070: 	backlog wait: 0.021277 	RTT: 0.127660 	total execute time: 0.148936 (milliseconds)
nid001070: READ:
nid001070: 	46 ops (11%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 2143
nid001070: 	backlog wait: 0.043478 	RTT: 0.195652 	total execute time: 0.239130 (milliseconds)
nid001070: READLINK:
nid001070: 	21 ops (5%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 132
nid001070: 	backlog wait: 0.000000 	RTT: 0.095238 	total execute time: 0.095238 (milliseconds)
nid001070: 
nid001070: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110212.cbqs01 /sfs :
nid001070: 
nid001070: Stats for 172.20.250.17:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001070:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001070:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001070:   NFS security flavor: 1  pseudoflavor: 0
nid001070: 
nid001070: NFS byte counts:
nid001070:   applications read 13993 bytes via read(2)
nid001070:   applications wrote 9215 bytes via write(2)
nid001070:   applications read 0 bytes via O_DIRECT read(2)
nid001070:   applications wrote 0 bytes via O_DIRECT write(2)
nid001070:   client read 4040 bytes via NFS READ
nid001070:   client wrote 78789 bytes via NFS WRITE
nid001070: 
nid001070: RPC statistics:
nid001070:   81 RPC requests sent, 81 RPC replies received (0 XIDs not found)
nid001070:   average backlog queue length: 0
nid001070: 
nid001070: GETATTR:
nid001070: 	86 ops (106%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 112
nid001070: 	backlog wait: 0.000000 	RTT: 0.127907 	total execute time: 0.162791 (milliseconds)
nid001070: WRITE:
nid001070: 	36 ops (44%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 160
nid001070: 	backlog wait: 0.055556 	RTT: 3.222222 	total execute time: 3.277778 (milliseconds)
nid001070: ACCESS:
nid001070: 	21 ops (25%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 120
nid001070: 	backlog wait: 0.000000 	RTT: 0.238095 	total execute time: 0.285714 (milliseconds)
nid001070: LOOKUP:
nid001070: 	5 ops (6%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 240
nid001070: 	backlog wait: 0.000000 	RTT: 0.200000 	total execute time: 0.200000 (milliseconds)
nid001070: SETATTR:
nid001070: 	3 ops (3%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 144
nid001070: 	backlog wait: 0.000000 	RTT: 0.333333 	total execute time: 0.333333 (milliseconds)
nid001070: READ:
nid001070: 	2 ops (2%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 2150
nid001070: 	backlog wait: 0.500000 	RTT: 0.500000 	total execute time: 0.500000 (milliseconds)
nid001070: 
nid001070: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110212.cbqs01 /u :
nid001070: 
nid001070: Stats for 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001070:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001070:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001070:   NFS security flavor: 1  pseudoflavor: 0
nid001070: 
nid001070: NFS byte counts:
nid001070:   applications read 837 bytes via read(2)
nid001070:   applications wrote 0 bytes via write(2)
nid001070:   applications read 0 bytes via O_DIRECT read(2)
nid001070:   applications wrote 0 bytes via O_DIRECT write(2)
nid001070:   client read 837 bytes via NFS READ
nid001070:   client wrote 0 bytes via NFS WRITE
nid001070: 
nid001070: RPC statistics:
nid001070:   81 RPC requests sent, 81 RPC replies received (0 XIDs not found)
nid001070:   average backlog queue length: 0
nid001070: 
nid001070: GETATTR:
nid001070: 	48 ops (59%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 112
nid001070: 	backlog wait: 0.000000 	RTT: 0.208333 	total execute time: 0.250000 (milliseconds)
nid001070: ACCESS:
nid001070: 	28 ops (34%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 120
nid001070: 	backlog wait: 0.000000 	RTT: 0.107143 	total execute time: 0.107143 (milliseconds)
nid001070: LOOKUP:
nid001070: 	12 ops (14%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 136
nid001070: 	backlog wait: 0.000000 	RTT: 0.166667 	total execute time: 0.250000 (milliseconds)
nid001070: READ:
nid001070: 	1 ops (1%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 968
nid001070: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.000000 (milliseconds)
nid001070: 
nid001070: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49110212.cbqs01 /pe :
nid001070: 
nid001070: Stats for 10.31.62.243:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001070:   NFS mount options: ro,vers=3,rsize=1048576,wsize=1048576,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,nolock,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.31.62.243,mountvers=3,mountport=38465,mountproto=tcp,local_lock=all
nid001070:   NFS server capabilities: caps=0x3fcf,wtmult=4096,dtsize=32768,bsize=0,namlen=255
nid001070:   NFS security flavor: 1  pseudoflavor: 0
nid001070: 
nid001070: NFS byte counts:
nid001070:   applications read 3072 bytes via read(2)
nid001070:   applications wrote 0 bytes via write(2)
nid001070:   applications read 0 bytes via O_DIRECT read(2)
nid001070:   applications wrote 0 bytes via O_DIRECT write(2)
nid001070:   client read 20480 bytes via NFS READ
nid001070:   client wrote 0 bytes via NFS WRITE
nid001070: 
nid001070: RPC statistics:
nid001070:   1281 RPC requests sent, 1281 RPC replies received (0 XIDs not found)
nid001070:   average backlog queue length: 0
nid001070: 
nid001070: GETATTR:
nid001070: 	829 ops (64%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 112
nid001070: 	backlog wait: 0.001206 	RTT: 0.448733 	total execute time: 0.462002 (milliseconds)
nid001070: LOOKUP:
nid001070: 	259 ops (20%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 62
nid001070: 	backlog wait: 0.003861 	RTT: 0.575290 	total execute time: 0.586873 (milliseconds)
nid001070: ACCESS:
nid001070: 	34 ops (2%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 36
nid001070: 	backlog wait: 0.000000 	RTT: 0.323529 	total execute time: 0.323529 (milliseconds)
nid001070: READLINK:
nid001070: 	2 ops (0%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 136
nid001070: 	backlog wait: 0.000000 	RTT: 0.500000 	total execute time: 0.000000 (milliseconds)
nid001070: READ:
nid001070: 	2 ops (0%) 
nid001070: 	avg bytes sent per op: avg bytes received per op: 10284
nid001070: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.500000 (milliseconds)
nid001070: 
nid001070: 

------------------------------------------------------------------
### Job 49110212.cbqs01 - Exit status is 0

------------------------------------------------------------------
Job 49110212.cbqs01 - Job summary:
Job Id: 49110212.cbqs01
    Job_Name = run_rtma_stats
    Job_Owner = emc.vpppg@clogin03.cactus.wcoss2.ncep.noaa.gov
    resources_used.cpupercent = 0
    resources_used.cput = 00:00:01
    resources_used.mem = 13260kb
    resources_used.ncpus = 1
    resources_used.vmem = 39096kb
    resources_used.walltime = 00:23:08
    job_state = R
    queue = dev
    server = cbqs01
    Account_Name = EVS-DEV
    Checkpoint = u
    ctime = Fri Mar 10 16:09:27 2023
    Error_Path = clogin03.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/noscrub
	/emc.vpppg/EVS/ecf/realtime_analyses/stats/run_rtma_stats.e49110212
    exec_host = nid001070/0
    exec_vnode = (nid001070:ncpus=1:mem=2097152kb)
    Hold_Types = n
    Join_Path = oe
    Keep_Files = oed
    Mail_Points = a
    mtime = Fri Mar 10 16:32:39 2023
    Output_Path = clogin03.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/noscru
	b/emc.vpppg/EVS/ecf/realtime_analyses/stats/run_rtma_stats.o49110212
    Priority = 0
    qtime = Fri Mar 10 16:09:27 2023
    Rerunable = False
    Resource_List.alvl = 2
    Resource_List.aslr = True
    Resource_List.debug = True
    Resource_List.dfs = False
    Resource_List.hyper = True
    Resource_List.mem = 2gb
    Resource_List.ncpus = 1
    Resource_List.nodect = 1
    Resource_List.one-shot = False
    Resource_List.place = shared
    Resource_List.select = 1:ncpus=1:mem=2GB
    Resource_List.thp = True
    Resource_List.turbo = True
    Resource_List.walltime = 02:00:00
    schedselect = 1:ncpus=1:mem=2GB:prepost=False
    stime = Fri Mar 10 16:09:28 2023
    session_id = 178437
    Shell_Path_List = /bin/bash
    jobdir = /u/emc.vpppg
    substate = 42
    Variable_List = PBS_O_HOME=/u/emc.vpppg,PBS_O_LANG=en_US.UTF-8,
	PBS_O_LOGNAME=emc.vpppg,
	PBS_O_PATH=/apps/spack/python/3.8.6/intel/19.1.3.304/pjn2nzkjvqgmjw4hm
	yz43v5x4jbxjzpk/bin:/pe/intel/compilers_and_libraries_2020.4.304/linux/
	bin/intel64:/pe/intel/compilers_and_libraries_2020.4.304/linux/bin:/pe/
	intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/pe/inte
	l/debugger_2020/gdb/intel64/bin:/opt/cray/libfabric/1.11.0.0./bin:/opt/
	clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/us
	r/bin:/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/apps/
	prod/python-modules/3.8.6/intel/19.1.3.304/bin:/apps/prod/python-module
	s/3.8.6/intel/19.1.3.304/lib/python3.8/site-packages/bin,
	PBS_O_MAIL=/var/mail/emc.vpppg,PBS_O_SHELL=/bin/bash,
	PBS_O_WORKDIR=/lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/realtime_ana
	lyses/stats,PBS_O_SYSTEM=Linux,PBS_O_QUEUE=dev,
	PBS_O_HOST=clogin03.cactus.wcoss2.ncep.noaa.gov
    euser = emc.vpppg
    egroup = emc
    hashname = 49110212.cbqs01
    queue_rank = 1678464567734485
    queue_type = E
    comment = Job run at Fri Mar 10 at 16:09 on (nid001070:ncpus=1:mem=2097152k
	b)
    etime = Fri Mar 10 16:09:27 2023
    umask = 22
    run_count = 6
    eligible_time = 00:00:04
    accrue_type = 3
    Submit_arguments = run_rtma_stats.sh
    project = EVS-DEV
    run_version = 1
    Submit_Host = clogin03.cactus.wcoss2.ncep.noaa.gov
    server_instance_id = cbqs01.cactus.wcoss2.ncep.noaa.gov:15001


------------------------------------------------------------------
Job 49110212.cbqs01 - PBS tracejob output (for parent mom node only):

Job: 49110212.cbqs01

03/10/2023 16:09:28  M    update_job_usage: CPU usage: 0.000 secs
03/10/2023 16:09:28  M    update_job_usage: cpupercent initialized to zero
03/10/2023 16:09:28  M    update_job_usage: Memory usage: mem=0b
03/10/2023 16:09:29  M    Started, pid = 178437
03/10/2023 16:10:39  M    update_job_usage: CPU usage: 0.683 secs
03/10/2023 16:10:39  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:12:40  M    update_job_usage: CPU usage: 0.719 secs
03/10/2023 16:12:40  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:14:41  M    update_job_usage: CPU usage: 0.754 secs
03/10/2023 16:14:41  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:16:42  M    update_job_usage: CPU usage: 0.791 secs
03/10/2023 16:16:42  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:18:43  M    update_job_usage: CPU usage: 0.827 secs
03/10/2023 16:18:43  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:20:44  M    update_job_usage: CPU usage: 0.865 secs
03/10/2023 16:20:44  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:22:46  M    update_job_usage: CPU usage: 0.902 secs
03/10/2023 16:22:46  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:24:48  M    update_job_usage: CPU usage: 0.938 secs
03/10/2023 16:24:48  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:26:49  M    update_job_usage: CPU usage: 0.973 secs
03/10/2023 16:26:49  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:28:50  M    update_job_usage: CPU usage: 1.009 secs
03/10/2023 16:28:50  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:30:51  M    update_job_usage: CPU usage: 1.045 secs
03/10/2023 16:30:51  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:32:52  M    update_job_usage: CPU usage: 1.082 secs
03/10/2023 16:32:52  M    update_job_usage: Memory usage: mem=13260kb
03/10/2023 16:33:34  M    task 00000001 terminated
03/10/2023 16:33:34  M    Terminated
03/10/2023 16:33:34  M    task 00000001 cput=00:00:02
03/10/2023 16:33:34  M    kill_job
03/10/2023 16:33:34  M    nid001070 cput=00:00:01 mem=13260kb
03/10/2023 16:33:34  M    update_job_usage: CPU usage: 1.083 secs
03/10/2023 16:33:34  M    update_job_usage: Memory usage: mem=13260kb

------------------------------------------------------------------
To see full PBS log data, run: /sfs/admin/scripts/tracejob.sh 49110212.cbqs01

==================================================================
END - DEBUG INFO
==================================================================

##### Job 49110212.cbqs01 - PBS Job Script:

#PBS -N run_rtma_stats
#PBS -j oe
#PBS -S /bin/bash
#PBS -q "dev"
#PBS -A EVS-DEV
#PBS -l walltime=02:00:00
#PBS -l select=1:ncpus=1:mem=2GB
#PBS -l debug=true

set -x

for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
do
   export fhr
   qsub -v cyc=$fhr /lfs/h2/emc/vpppg/noscrub/$USER/EVS/ecf/realtime_analyses/stats/jevs_rtma_stats.ecf
   sleep 60
done

exit

##### End of job script
------------------------------------------------------------------
