Running prologue on parent mom node: nid001301...
Job 49356444.cbqs01 nodelist: nid001301
Job 49356444.cbqs01 - Prologue complete. Execution time: 1 seconds
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=00 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356555.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=01 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356562.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=02 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356591.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=03 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356628.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=04 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356815.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=05 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356903.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=06 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356940.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=07 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356955.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=08 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49356971.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=09 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357081.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=10 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357154.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=11 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357185.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=12 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357234.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=13 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357248.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=14 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357603.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=15 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357622.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=16 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357652.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=17 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357666.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=18 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357723.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=19 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357790.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=20 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357945.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=21 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357955.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=22 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49357986.cbqs01
++ sleep 60
++ for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
++ export fhr
++ qsub -v cyc=23 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
49358027.cbqs01
++ sleep 60
++ exit
Job 49356444.cbqs01 - Epilogue complete. Execution time: 1 seconds
==================================================================
BEGIN - DEBUG INFO
==================================================================
Job 49356444.cbqs01 - Post Job Mem Usage:
nid001301: 2023-03-13.43 1 ===========================================
nid001301: 2023-03-13.43 2 #### Node Memory Usage for nid001301 ####
nid001301: 2023-03-13.43 3 ------------------
nid001301: 2023-03-13.43 4 Mem: total:527073140 used:11782316 free:515970176 shared:382300 cache:1788772 avail:515290824
nid001301: 2023-03-13.43 5 4.0K	/dev/shm
nid001301: 2023-03-13.43 6 7.6M	/tmp
nid001301: 2023-03-13.43 7 ===========================================
nid001301: 2023-03-13.50 1 ===========================================
nid001301: 2023-03-13.50 2 #### Node Memory Usage for nid001301 ####
nid001301: 2023-03-13.50 3 ------------------
nid001301: 2023-03-13.50 4 Mem: total:527073140 used:11796080 free:515904836 shared:382276 cache:1891552 avail:515277060
nid001301: 2023-03-13.50 5 4.0K	/dev/shm
nid001301: 2023-03-13.50 6 7.6M	/tmp
nid001301: 2023-03-13.50 7 ===========================================

------------------------------------------------------------------
Job 49356444.cbqs01 - /var/log/messages for job duration:
nid001301: 2023-03-13A--:--:--.------+--:-- nid001301 #### nid001301 - Job 49356444.cbqs01 Runtime Data from /var/log/messages
nid001301: 2023-03-13T15:46:42.981512+00:00 nid001301 prologue: MARK: Job 49356444.cbqs01 Start
nid001301: 2023-03-13T15:46:42.987726+00:00 nid001301 prologue: Job 49356444.cbqs01 nodelist: nid001301
nid001301: 2023-03-13T15:46:42.988870+00:00 nid001301 prologue: Job 49356444.cbqs01 - Checking existing ASLR settings...
nid001301: 2023-03-13T15:46:42.994636+00:00 nid001301 prologue: Job 49356444.cbqs01 - Checking palsd open file count...
nid001301: 2023-03-13T15:46:43.100880+00:00 nid001301 prologue: Job 49356444.cbqs01 - Checking one-shot control: False
nid001301: 2023-03-13T15:46:43.101926+00:00 nid001301 prologue: Job 49356444.cbqs01 - Recording pre-job HSN counters...
nid001301: 2023-03-13T15:46:43.209153+00:00 nid001301 prologue: Job 49356444.cbqs01 - Recording pre-job memory usage...
nid001301: 2023-03-13T15:46:43.268100+00:00 nid001301 prologue: Job 49356444.cbqs01 - Killing any stray user processes...
nid001301: 2023-03-13T15:46:43.341633+00:00 nid001301 prologue: Job 49356444.cbqs01 - Enabling turboboost...
nid001301: 2023-03-13T15:46:43.344104+00:00 nid001301 prologue: Job 49356444.cbqs01 - Verifying post-boot workarounds...
nid001301: 2023-03-13T15:46:43.646806+00:00 nid001301 prologue: Job 49356444.cbqs01 - Warchk Complete
nid001301: 2023-03-13T15:46:43.648403+00:00 nid001301 prologue: Job 49356444.cbqs01 - Recording initial NFS client statistics...
nid001301: 2023-03-13T15:46:43.657177+00:00 nid001301 prologue: Job 49356444.cbqs01 - Prologue complete. Execution time: 1 seconds
nid001301: 2023-03-13T15:46:45.056747+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=00 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:47:45.130771+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=01 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:48:45.259331+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=02 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:49:45.302549+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=03 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:50:45.357852+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=04 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:51:45.399954+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=05 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:52:22.758751+00:00 nid001301 systemd[1]: etc_update.service: Succeeded.
nid001301: 2023-03-13T15:52:45.438754+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=06 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:53:45.478827+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=07 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:54:45.518309+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=08 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:55:45.558756+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=09 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:56:45.850813+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=10 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:57:45.891672+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=11 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:58:45.933074+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=12 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T15:59:45.981402+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=13 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:00:02.733163+00:00 nid001301 systemd[1]: Started Timeline of Snapper Snapshots.
nid001301: 2023-03-13T16:00:02.871350+00:00 nid001301 dbus-daemon[5735]: [system] Activating via systemd: service name='org.opensuse.Snapper' unit='snapperd.service' requested by ':1.21341' (uid=0 pid=5913 comm="/usr/lib/snapper/systemd-helper --timeline ")
nid001301: 2023-03-13T16:00:02.874778+00:00 nid001301 systemd[1]: Starting DBus interface for snapper...
nid001301: 2023-03-13T16:00:02.887930+00:00 nid001301 dbus-daemon[5735]: [system] Successfully activated service 'org.opensuse.Snapper'
nid001301: 2023-03-13T16:00:02.888016+00:00 nid001301 systemd[1]: Started DBus interface for snapper.
nid001301: 2023-03-13T16:00:02.898971+00:00 nid001301 systemd[1]: snapper-timeline.service: Succeeded.
nid001301: 2023-03-13T16:00:11.792758+00:00 nid001301 check_and_pull_files.sh[6000]: d3af83807b87c31a6411ce051209786cd18e3c72  passwd
nid001301: 2023-03-13T16:00:11.792858+00:00 nid001301 check_and_pull_files.sh[6000]: 3aadcc0378aceb4979e4333a88353b1c8679e06b  group
nid001301: 2023-03-13T16:00:11.793252+00:00 nid001301 check_and_pull_files.sh[6000]: dd1ec1f03e369ce9efb173ba2718f362db21093d  hosts
nid001301: 2023-03-13T16:00:11.795257+00:00 nid001301 check_and_pull_files.sh[6000]: 0c1b2d0b06d036dc9b15eaedad6d52308eb7e695  shadow
nid001301: 2023-03-13T16:00:11.796814+00:00 nid001301 systemd[1]: etc_update.service: Succeeded.
nid001301: 2023-03-13T16:00:46.021586+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=14 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:01:02.960979+00:00 nid001301 systemd[1]: snapperd.service: Succeeded.
nid001301: 2023-03-13T16:01:46.357578+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=15 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:02:46.398647+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=16 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:03:46.442307+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=17 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:04:46.482280+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=18 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:05:49.492335+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=19 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:06:49.546924+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=20 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:07:49.659036+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=21 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:08:49.698547+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=22 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:09:49.739297+00:00 nid001301 PBS_CMD: emc.vpppg : /opt/pbs/bin/qsub -v cyc=23 /lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
nid001301: 2023-03-13T16:10:50.368349+00:00 nid001301 epilogue: Job 49356444.cbqs01 complete, running post-job actions.
nid001301: 2023-03-13T16:10:50.384356+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Recording post-job HSN counters...
nid001301: 2023-03-13T16:10:50.495018+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Recording post-job memory usage...
nid001301: 2023-03-13T16:10:50.510514+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Recording job NFS statistics to /tmp/nfsstats.49356444.cbqs01
nid001301: 2023-03-13T16:10:50.987810+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Clearing /tmp...
nid001301: 2023-03-13T16:10:51.004408+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Clearing shared memory...
nid001301: 2023-03-13T16:10:51.015817+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Clearing memory cache...
nid001301: 2023-03-13T16:10:51.162109+00:00 nid001301 kernel: [836466.156354] drop_caches (6335): drop_caches: 3
nid001301: 2023-03-13T16:10:51.271777+00:00 nid001301 epilogue: Job 49356444.cbqs01 - Releasing Lustre Locks...
nid001301: 2023-03-13T16:10:51.285322+00:00 nid001301 epilogue: MARK: Job 49356444.cbqs01 Complete.

------------------------------------------------------------------
Job 49356444.cbqs01 - dmesg output for job duration:
nid001301: 2023-03-13A--:--:--.------+--:-- nid001301 #### nid001301 - Job 49356444.cbqs01 Runtime Data from dmesg
nid001301: [Mon Mar 13 15:46:33 2023] MARK: Job 49356444.cbqs01 Start
nid001301: [Mon Mar 13 16:10:41 2023] MARK: Job 49356444.cbqs01 Complete.
nid001301: [Mon Mar 13 16:10:41 2023] drop_caches (6335): drop_caches: 3

------------------------------------------------------------------
Job 49356444.cbqs01 - Pre/Post job diff on HSN (MLX) Counters:
nid001301: 2023-03-13A--:--:--.------+--:-- nid001301 #### nid001301 - Job 49356444.cbqs01 HSN0 MLX Counter Post-Job Difference
nid001301: multicast: 257392266					      |	multicast: 257396743
nid001301: port_rcv_data: 38410041370176				      |	port_rcv_data: 38410042168631
nid001301: port_rcv_packets: 53331352245				      |	port_rcv_packets: 53331365819
nid001301: port_xmit_data: 37580250799289				      |	port_xmit_data: 37580251615987
nid001301: port_xmit_packets: 52068632938				      |	port_xmit_packets: 52068646584
nid001301: rx_bytes: 85600871312					      |	rx_bytes: 85602105839
nid001301: rx_packets: 293108313					      |	rx_packets: 293115513
nid001301: tx_bytes: 63822753829					      |	tx_bytes: 63823442226
nid001301: tx_packets: 31881974					      |	tx_packets: 31885732
nid001301: unicast_rcv_packets: 53331352245			      |	unicast_rcv_packets: 53331365819
nid001301: unicast_xmit_packets: 52068632938			      |	unicast_xmit_packets: 52068646584

------------------------------------------------------------------
### Job 49356444.cbqs01 - NFS Statistics for job duration:
nid001301: #### 2023-03-13.00 #### nid001301 - Job 49356444.cbqs01 NFS Statistics for job duration (nfsstat)
nid001301: ## /usr/sbin/nfsstat -v -S /tmp/nfsstats.begin.49356444.cbqs01 :
nid001301: Client packet stats:
nid001301: packets    udp        tcp        tcpconn
nid001301: 0          0          0          0       
nid001301: 
nid001301: Client rpc stats:
nid001301: calls      retrans    authrefrsh
nid001301: 2895       0          2895    
nid001301: 
nid001301: Client nfs v3:
nid001301: null             getattr          setattr          lookup           access           
nid001301: 0         0%     1679     57%     3         0%     363      12%     297      10%     
nid001301: readlink         read             write            create           mkdir            
nid001301: 30        1%     385      13%     36        1%     0         0%     0         0%     
nid001301: symlink          mknod            remove           rmdir            rename           
nid001301: 0         0%     0         0%     0         0%     0         0%     0         0%     
nid001301: link             readdir          readdirplus      fsstat           fsinfo           
nid001301: 0         0%     0         0%     102       3%     0         0%     0         0%     
nid001301: pathconf         commit           
nid001301: 0         0%     0         0%     
nid001301: 
nid001301: 
nid001301: -----------------------------------------------------------------------------------
nid001301: 
nid001301: #### 2023-03-13.00 #### nid001301 - Job 49356444.cbqs01 NFS Mount Statistics for job duration (mountstats)
nid001301: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49356444.cbqs01 /apps :
nid001301: 
nid001301: 
nid001301: 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001301: 
nid001301:            ops/s       rpc bklog
nid001301:            0.283           0.000
nid001301: 
nid001301: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.059           0.149           2.529        0 (0.0%)           0.247           0.294
nid001301: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001301: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49356444.cbqs01 /sfs :
nid001301: 
nid001301: 
nid001301: 172.20.250.17:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001301: 
nid001301:            ops/s       rpc bklog
nid001301:            0.057           0.000
nid001301: 
nid001301: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.003           0.102          29.505        0 (0.0%)           0.600           0.600
nid001301: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.025           0.061           2.445        0 (0.0%)           0.333           0.444
nid001301: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49356444.cbqs01 /u :
nid001301: 
nid001301: 
nid001301: 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001301: 
nid001301:            ops/s       rpc bklog
nid001301:            0.057           0.000
nid001301: 
nid001301: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.001           0.001           1.098        0 (0.0%)           0.000           0.000
nid001301: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001301: ## /usr/sbin/mountstats iostat -S /tmp/mntstats.begin.49356444.cbqs01 /pe :
nid001301: 
nid001301: 
nid001301: 10.31.62.244:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001301: 
nid001301:            ops/s       rpc bklog
nid001301:            1.051           0.000
nid001301: 
nid001301: read:              ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.024           1.798          74.351        0 (0.0%)           1.514           1.543
nid001301: write:             ops/s            kB/s           kB/op         retrans    avg RTT (ms)    avg exe (ms)
nid001301:                    0.000           0.000           0.000        0 (0.0%)           0.000           0.000
nid001301: ----------------
nid001301: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49356444.cbqs01 /apps :
nid001301: Stats for 172.20.250.16:/AZ-HFS-Cactus-apps mounted on /apps:
nid001301:   NFS mount options: ro,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.16,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001301:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001301:   NFS security flavor: 1  pseudoflavor: 0
nid001301: 
nid001301: NFS byte counts:
nid001301:   applications read 497530 bytes via read(2)
nid001301:   applications wrote 0 bytes via write(2)
nid001301:   applications read 0 bytes via O_DIRECT read(2)
nid001301:   applications wrote 0 bytes via O_DIRECT write(2)
nid001301:   client read 195898 bytes via NFS READ
nid001301:   client wrote 0 bytes via NFS WRITE
nid001301: 
nid001301: RPC statistics:
nid001301:   410 RPC requests sent, 410 RPC replies received (0 XIDs not found)
nid001301:   average backlog queue length: 0
nid001301: 
nid001301: GETATTR:
nid001301: 	718 ops (175%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 112
nid001301: 	backlog wait: 0.002786 	RTT: 0.097493 	total execute time: 0.110028 (milliseconds)
nid001301: ACCESS:
nid001301: 	214 ops (52%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 120
nid001301: 	backlog wait: 0.004673 	RTT: 0.303738 	total execute time: 0.308411 (milliseconds)
nid001301: READDIRPLUS:
nid001301: 	102 ops (24%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 876
nid001301: 	backlog wait: 0.009804 	RTT: 0.107843 	total execute time: 0.117647 (milliseconds)
nid001301: READ:
nid001301: 	85 ops (20%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 2434
nid001301: 	backlog wait: 0.035294 	RTT: 0.247059 	total execute time: 0.294118 (milliseconds)
nid001301: LOOKUP:
nid001301: 	83 ops (20%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 169
nid001301: 	backlog wait: 0.000000 	RTT: 0.108434 	total execute time: 0.120482 (milliseconds)
nid001301: READLINK:
nid001301: 	28 ops (6%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 137
nid001301: 	backlog wait: 0.000000 	RTT: 0.107143 	total execute time: 0.107143 (milliseconds)
nid001301: 
nid001301: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49356444.cbqs01 /sfs :
nid001301: 
nid001301: Stats for 172.20.250.17:/AZ-HFS-Cactus-sfs mounted on /sfs:
nid001301:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001301:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001301:   NFS security flavor: 1  pseudoflavor: 0
nid001301: 
nid001301: NFS byte counts:
nid001301:   applications read 163333 bytes via read(2)
nid001301:   applications wrote 9333 bytes via write(2)
nid001301:   applications read 0 bytes via O_DIRECT read(2)
nid001301:   applications wrote 0 bytes via O_DIRECT write(2)
nid001301:   client read 149778 bytes via NFS READ
nid001301:   client wrote 79274 bytes via NFS WRITE
nid001301: 
nid001301: RPC statistics:
nid001301:   82 RPC requests sent, 82 RPC replies received (0 XIDs not found)
nid001301:   average backlog queue length: 0
nid001301: 
nid001301: GETATTR:
nid001301: 	85 ops (103%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 112
nid001301: 	backlog wait: 0.000000 	RTT: 0.141176 	total execute time: 0.164706 (milliseconds)
nid001301: WRITE:
nid001301: 	36 ops (43%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 160
nid001301: 	backlog wait: 0.027778 	RTT: 0.333333 	total execute time: 0.444444 (milliseconds)
nid001301: ACCESS:
nid001301: 	26 ops (31%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 120
nid001301: 	backlog wait: 0.000000 	RTT: 0.269231 	total execute time: 0.269231 (milliseconds)
nid001301: LOOKUP:
nid001301: 	9 ops (10%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 240
nid001301: 	backlog wait: 0.000000 	RTT: 0.222222 	total execute time: 0.222222 (milliseconds)
nid001301: READ:
nid001301: 	5 ops (6%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 30084
nid001301: 	backlog wait: 0.000000 	RTT: 0.600000 	total execute time: 0.600000 (milliseconds)
nid001301: SETATTR:
nid001301: 	3 ops (3%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 144
nid001301: 	backlog wait: 0.000000 	RTT: 0.333333 	total execute time: 0.333333 (milliseconds)
nid001301: 
nid001301: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49356444.cbqs01 /u :
nid001301: 
nid001301: Stats for 172.20.250.17:/AZ-HFS-Cactus-u mounted on /u:
nid001301:   NFS mount options: rw,vers=3,rsize=65536,wsize=65536,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,proto=tcp,nconnect=3,timeo=600,retrans=2,sec=sys,mountaddr=172.20.250.17,mountvers=3,mountport=4048,mountproto=udp,local_lock=none
nid001301:   NFS server capabilities: caps=0x3fc7,wtmult=4096,dtsize=8192,bsize=0,namlen=255
nid001301:   NFS security flavor: 1  pseudoflavor: 0
nid001301: 
nid001301: NFS byte counts:
nid001301:   applications read 837 bytes via read(2)
nid001301:   applications wrote 0 bytes via write(2)
nid001301:   applications read 0 bytes via O_DIRECT read(2)
nid001301:   applications wrote 0 bytes via O_DIRECT write(2)
nid001301:   client read 837 bytes via NFS READ
nid001301:   client wrote 0 bytes via NFS WRITE
nid001301: 
nid001301: RPC statistics:
nid001301:   82 RPC requests sent, 82 RPC replies received (0 XIDs not found)
nid001301:   average backlog queue length: 0
nid001301: 
nid001301: GETATTR:
nid001301: 	48 ops (58%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 112
nid001301: 	backlog wait: 0.000000 	RTT: 0.229167 	total execute time: 0.250000 (milliseconds)
nid001301: ACCESS:
nid001301: 	23 ops (28%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 120
nid001301: 	backlog wait: 0.000000 	RTT: 0.130435 	total execute time: 0.173913 (milliseconds)
nid001301: LOOKUP:
nid001301: 	12 ops (14%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 136
nid001301: 	backlog wait: 0.000000 	RTT: 0.083333 	total execute time: 0.083333 (milliseconds)
nid001301: READ:
nid001301: 	1 ops (1%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 968
nid001301: 	backlog wait: 0.000000 	RTT: 0.000000 	total execute time: 0.000000 (milliseconds)
nid001301: 
nid001301: ## /usr/sbin/mountstats mountstats -S /tmp/mntstats.begin.49356444.cbqs01 /pe :
nid001301: 
nid001301: Stats for 10.31.62.244:/cm_shared/image/images_rw_nfs/pe mounted on /pe:
nid001301:   NFS mount options: ro,vers=3,rsize=1048576,wsize=1048576,namlen=255,acregmin=3,acregmax=60,acdirmin=30,acdirmax=60,hard,nolock,proto=tcp,timeo=600,retrans=2,sec=sys,mountaddr=10.31.62.244,mountvers=3,mountport=38465,mountproto=tcp,local_lock=all
nid001301:   NFS server capabilities: caps=0x3fcf,wtmult=4096,dtsize=32768,bsize=0,namlen=255
nid001301:   NFS security flavor: 1  pseudoflavor: 0
nid001301: 
nid001301: NFS byte counts:
nid001301:   applications read 432128 bytes via read(2)
nid001301:   applications wrote 0 bytes via write(2)
nid001301:   applications read 0 bytes via O_DIRECT read(2)
nid001301:   applications wrote 0 bytes via O_DIRECT write(2)
nid001301:   client read 2658304 bytes via NFS READ
nid001301:   client wrote 0 bytes via NFS WRITE
nid001301: 
nid001301: RPC statistics:
nid001301:   1521 RPC requests sent, 1521 RPC replies received (0 XIDs not found)
nid001301:   average backlog queue length: 0
nid001301: 
nid001301: GETATTR:
nid001301: 	828 ops (54%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 112
nid001301: 	backlog wait: 0.001208 	RTT: 0.399758 	total execute time: 0.411836 (milliseconds)
nid001301: LOOKUP:
nid001301: 	259 ops (17%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 62
nid001301: 	backlog wait: 0.000000 	RTT: 0.586873 	total execute time: 0.594595 (milliseconds)
nid001301: READ:
nid001301: 	35 ops (2%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 75995
nid001301: 	backlog wait: 0.028571 	RTT: 1.514286 	total execute time: 1.542857 (milliseconds)
nid001301: ACCESS:
nid001301: 	34 ops (2%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 36
nid001301: 	backlog wait: 0.000000 	RTT: 0.323529 	total execute time: 0.323529 (milliseconds)
nid001301: READLINK:
nid001301: 	2 ops (0%) 
nid001301: 	avg bytes sent per op: avg bytes received per op: 136
nid001301: 	backlog wait: 0.000000 	RTT: 0.500000 	total execute time: 0.500000 (milliseconds)
nid001301: 
nid001301: 

------------------------------------------------------------------
### Job 49356444.cbqs01 - Exit status is 0

------------------------------------------------------------------
Job 49356444.cbqs01 - Job summary:
Job Id: 49356444.cbqs01
    Job_Name = run_rtma_stats
    Job_Owner = emc.vpppg@clogin03.cactus.wcoss2.ncep.noaa.gov
    resources_used.cpupercent = 1
    resources_used.cput = 00:00:01
    resources_used.mem = 22720kb
    resources_used.ncpus = 1
    resources_used.vmem = 39096kb
    resources_used.walltime = 00:23:08
    job_state = R
    queue = dev
    server = cbqs01
    Account_Name = EVS-DEV
    Checkpoint = u
    ctime = Mon Mar 13 15:44:40 2023
    Error_Path = clogin03.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/noscrub
	/emc.vpppg/EVS/ecf/analyses/stats/run_rtma_stats.e49356444
    exec_host = nid001301/0
    exec_vnode = (nid001301:ncpus=1:mem=2097152kb)
    Hold_Types = n
    Join_Path = oe
    Keep_Files = oed
    Mail_Points = a
    mtime = Mon Mar 13 16:09:53 2023
    Output_Path = clogin03.cactus.wcoss2.ncep.noaa.gov:/lfs/h2/emc/vpppg/noscru
	b/emc.vpppg/EVS/ecf/analyses/stats/run_rtma_stats.o49356444
    Priority = 0
    qtime = Mon Mar 13 15:44:40 2023
    Rerunable = False
    Resource_List.alvl = 2
    Resource_List.aslr = True
    Resource_List.debug = True
    Resource_List.dfs = False
    Resource_List.hyper = True
    Resource_List.mem = 2gb
    Resource_List.ncpus = 1
    Resource_List.nodect = 1
    Resource_List.one-shot = False
    Resource_List.place = shared
    Resource_List.select = 1:ncpus=1:mem=2GB
    Resource_List.thp = True
    Resource_List.turbo = True
    Resource_List.walltime = 02:00:00
    schedselect = 1:ncpus=1:mem=2GB:prepost=False
    stime = Mon Mar 13 15:46:42 2023
    session_id = 5350
    Shell_Path_List = /bin/bash
    jobdir = /u/emc.vpppg
    substate = 42
    Variable_List = PBS_O_HOME=/u/emc.vpppg,PBS_O_LANG=en_US.UTF-8,
	PBS_O_LOGNAME=emc.vpppg,
	PBS_O_PATH=/apps/spack/python/3.8.6/intel/19.1.3.304/pjn2nzkjvqgmjw4hm
	yz43v5x4jbxjzpk/bin:/pe/intel/compilers_and_libraries_2020.4.304/linux/
	bin/intel64:/pe/intel/compilers_and_libraries_2020.4.304/linux/bin:/pe/
	intel/compilers_and_libraries_2020.4.304/linux/mpi/intel64/bin:/pe/inte
	l/debugger_2020/gdb/intel64/bin:/opt/cray/libfabric/1.11.0.0./bin:/opt/
	clmgr/sbin:/opt/clmgr/bin:/opt/sgi/sbin:/opt/sgi/bin:/usr/local/bin:/us
	r/bin:/bin:/usr/lib/mit/bin:/usr/lib/mit/sbin:/opt/pbs/bin:/sbin:/apps/
	prod/python-modules/3.8.6/intel/19.1.3.304/bin:/apps/prod/python-module
	s/3.8.6/intel/19.1.3.304/lib/python3.8/site-packages/bin,
	PBS_O_MAIL=/var/mail/emc.vpppg,PBS_O_SHELL=/bin/bash,
	PBS_O_WORKDIR=/lfs/h2/emc/vpppg/noscrub/emc.vpppg/EVS/ecf/analyses/sta
	ts,PBS_O_SYSTEM=Linux,PBS_O_QUEUE=dev,
	PBS_O_HOST=clogin03.cactus.wcoss2.ncep.noaa.gov
    euser = emc.vpppg
    egroup = emc
    hashname = 49356444.cbqs01
    queue_rank = 1678722280307528
    queue_type = E
    comment = Job run at Mon Mar 13 at 15:46 on (nid001301:ncpus=1:mem=2097152k
	b)
    etime = Mon Mar 13 15:44:40 2023
    umask = 22
    run_count = 6
    eligible_time = 00:02:05
    accrue_type = 3
    Submit_arguments = run_rtma_stats.sh
    project = EVS-DEV
    run_version = 1
    Submit_Host = clogin03.cactus.wcoss2.ncep.noaa.gov
    server_instance_id = cbqs01.cactus.wcoss2.ncep.noaa.gov:15001


------------------------------------------------------------------
Job 49356444.cbqs01 - PBS tracejob output (for parent mom node only):

Job: 49356444.cbqs01

03/13/2023 15:46:42  M    update_job_usage: CPU usage: 0.000 secs
03/13/2023 15:46:42  M    update_job_usage: cpupercent initialized to zero
03/13/2023 15:46:42  M    update_job_usage: Memory usage: mem=0b
03/13/2023 15:46:43  M    Started, pid = 5350
03/13/2023 15:47:47  M    update_job_usage: CPU usage: 0.767 secs
03/13/2023 15:47:47  M    update_job_usage: 49356444.cbqs01 measured interval cpupercent 1 increased job cpupercent to 1
03/13/2023 15:47:47  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 15:49:48  M    update_job_usage: CPU usage: 0.804 secs
03/13/2023 15:49:48  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 15:51:49  M    update_job_usage: CPU usage: 0.840 secs
03/13/2023 15:51:49  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 15:53:51  M    update_job_usage: CPU usage: 0.875 secs
03/13/2023 15:53:51  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 15:55:52  M    update_job_usage: CPU usage: 0.911 secs
03/13/2023 15:55:52  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 15:57:54  M    update_job_usage: CPU usage: 0.948 secs
03/13/2023 15:57:54  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 15:59:55  M    update_job_usage: CPU usage: 0.984 secs
03/13/2023 15:59:55  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 16:01:56  M    update_job_usage: CPU usage: 1.021 secs
03/13/2023 16:01:56  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 16:03:57  M    update_job_usage: CPU usage: 1.057 secs
03/13/2023 16:03:57  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 16:05:58  M    update_job_usage: CPU usage: 1.094 secs
03/13/2023 16:05:58  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 16:07:59  M    update_job_usage: CPU usage: 1.131 secs
03/13/2023 16:07:59  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 16:10:00  M    update_job_usage: CPU usage: 1.168 secs
03/13/2023 16:10:00  M    update_job_usage: Memory usage: mem=22720kb
03/13/2023 16:10:49  M    task 00000001 terminated
03/13/2023 16:10:49  M    Terminated
03/13/2023 16:10:49  M    task 00000001 cput=00:00:02
03/13/2023 16:10:49  M    kill_job
03/13/2023 16:10:49  M    nid001301 cput=00:00:01 mem=22720kb
03/13/2023 16:10:49  M    update_job_usage: CPU usage: 1.169 secs
03/13/2023 16:10:49  M    update_job_usage: Memory usage: mem=22720kb

------------------------------------------------------------------
To see full PBS log data, run: /sfs/admin/scripts/tracejob.sh 49356444.cbqs01

==================================================================
END - DEBUG INFO
==================================================================

##### Job 49356444.cbqs01 - PBS Job Script:

#PBS -N run_rtma_stats
#PBS -j oe
#PBS -S /bin/bash
#PBS -q "dev"
#PBS -A EVS-DEV
#PBS -l walltime=02:00:00
#PBS -l select=1:ncpus=1:mem=2GB
#PBS -l debug=true

set -x

for fhr in 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 21 22 23
do
   export fhr
   qsub -v cyc=$fhr /lfs/h2/emc/vpppg/noscrub/$USER/EVS/ecf/analyses/stats/jevs_rtma_stats.ecf
   sleep 60
done

exit

##### End of job script
------------------------------------------------------------------
